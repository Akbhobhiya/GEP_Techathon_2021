{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemma tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "def get_content_as_string(document_path):\n",
    "    txt = Path(document_path).read_text()\n",
    "    txt = txt.replace('\\n', ' ')\n",
    "    txt = re.sub('\\W+', ' ', txt) #Select only alpha numerics\n",
    "    txt = re.sub('[^A-Za-z]+', ' ', txt) #select only alphabet characters\n",
    "    txt = txt.lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize_document(text_file):\n",
    "    tokens = nltk.word_tokenize(text_file)\n",
    "    return tokens\n",
    "\n",
    "def tag_tokens(tokens):\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_idf_lemmetizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "def stemmed_words(doc):\n",
    "    return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in set(stp.words('english')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_binay_cosine_similarity(compare_doc,doc_corpus):\n",
    "    count_vect = CountVectorizer(binary=True,analyzer=stemmed_words)\n",
    "    cv_req_vector = count_vect.fit_transform([compare_doc]).todense()\n",
    "    print('Features are:' ,count_vect.get_feature_names())\n",
    "    cv_resume_vector = count_vect.transform(doc_corpus).todense()\n",
    "    cosine_similarity_list = []\n",
    "    for i in range(len(cv_resume_vector)):\n",
    "        cosine_similarity_list.append(cosine_similarity(cv_req_vector,cv_resume_vector[i])[0][0])\n",
    "    return cosine_similarity_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf_idf_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as stp\n",
    "\n",
    "def get_tf_idf_cosine_similarity(compare_doc,doc_corpus):\n",
    "    tf_idf_vect = TfidfVectorizer(analyzer=stemmed_words)\n",
    "    tf_idf_req_vector = tf_idf_vect.fit_transform([compare_doc]).todense()\n",
    "    tf_idf_resume_vector = tf_idf_vect.transform(doc_corpus).todense()\n",
    "    cosine_similarity_list = []\n",
    "    for i in range(len(tf_idf_resume_vector)):\n",
    "        cosine_similarity_list.append(cosine_similarity(tf_idf_req_vector,tf_idf_resume_vector[i])[0][0])\n",
    "    return cosine_similarity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resume_matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def process_files(req_document,resume_docs):\n",
    "    req_doc_text = get_content_as_string(req_document)\n",
    "    resume_doc_text = []\n",
    "    for doct in resume_docs:\n",
    "        resume_doc_text.append(get_content_as_string(doct))\n",
    "\n",
    "    cos_sim_list = get_tf_idf_cosine_similarity(req_doc_text,resume_doc_text)\n",
    "    print(cos_sim_list)\n",
    "    final_doc_rating_list = []\n",
    "    zipped_docs = zip(cos_sim_list,resume_docs)\n",
    "    sorted_doc_list = sorted(zipped_docs, key = lambda x: x[0], reverse=True)\n",
    "    for element in sorted_doc_list:\n",
    "        doc_rating_list = []\n",
    "        doc_rating_list.append(os.path.basename(element[1]))\n",
    "        doc_rating_list.append(\"{:.0%}\".format(element[0]))\n",
    "        final_doc_rating_list.append(doc_rating_list)\n",
    "    return final_doc_rating_list\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     req_document = '/home/ashok/Desktop/GEP_Techathon/jobdescription/req.txt'\n",
    "     resume_docs = ['/home/ashok/Desktop/GEP_Techathon/resume/resume1.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume2.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume3.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume4.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume5.txt']\n",
    "     final_doc_rating_list=process_files(req_document,resume_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc_rating_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Extraction from Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('NAME',[pattern])\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_resume_txt(document_path):\n",
    "    txt = Path(document_path).read_text()\n",
    "    txt = txt.replace('\\n', ' ')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resume_docs = ['/home/ashok/Desktop/GEP_Techathon/resume/resume1.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume2.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume3.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume4.txt',\n",
    "                    '/home/ashok/Desktop/GEP_Techathon/resume/resume5.txt']\n",
    "for doc in resume_docs:\n",
    "    resume_txt = get_resume_txt(doc)\n",
    "    name = extract_name(resume_txt)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Extraction from Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in resume_docs:\n",
    "    resume_txt = get_resume_txt(doc)\n",
    "    email = extract_emails(resume_txt)\n",
    "    print(email[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
